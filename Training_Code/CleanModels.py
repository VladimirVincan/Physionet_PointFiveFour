import torch
from ModelDefinition import Sleep_model_MultiTarget, Normalizer, SeperableDenseNetUnit, SkipLSTM
import numpy as np
from ExtractAllData import extractWholeRecord

'''
Script which cleans up models (fixes issue with non-integer padding, registers
batch normalization buffers and parameters, and extracts state_dict).
Verifies that the model predictions match after cleaning and on CPU and GPU to within a small tolerance.
Should be run on models that have been generated by TrainModel
'''

def cleanModel(model):
    assert isinstance(model, Sleep_model_MultiTarget)
    model.eval()

    for module in model.modules():

        # Force padding to be integer valued
        if hasattr(module, 'padding'):
            module.padding = (int(module.padding[0]),)

        # Register necessary parameters for state_dict to work properly
        if isinstance(module, Normalizer):
            x = module.movingAverage
            delattr(module, 'movingAverage')
            module.register_buffer('movingAverage', x)

            x = module.movingVariance
            delattr(module, 'movingVariance')
            module.register_buffer('movingVariance', x)

            x = torch.nn.Parameter(module.BatchNormScale)
            delattr(module, 'BatchNormScale')
            module.register_parameter('BatchNormScale', x)

            x = torch.nn.Parameter(module.BatchNormBias)
            delattr(module, 'BatchNormBias')
            module.register_parameter('BatchNormBias', x)

        # Remove weight normalization
        if isinstance(module, SeperableDenseNetUnit):
            module.conv1 = torch.nn.utils.remove_weight_norm(module.conv1, 'weight')
            module.conv2 = torch.nn.utils.remove_weight_norm(module.conv2, 'weight')
            module.conv3 = torch.nn.utils.remove_weight_norm(module.conv3, 'weight')
            module.conv4 = torch.nn.utils.remove_weight_norm(module.conv4, 'weight')

        if isinstance(module, SkipLSTM):
            module.outputConv1 = torch.nn.utils.remove_weight_norm(module.outputConv1, name='weight')
            module.outputConv2 = torch.nn.utils.remove_weight_norm(module.outputConv2, name='weight')

            module.rnn = torch.nn.utils.remove_weight_norm(module.rnn, name='weight_ih_l0')
            module.rnn = torch.nn.utils.remove_weight_norm(module.rnn, name='weight_hh_l0')

    return model


modelPaths = ['../TrainedModels/Model_Auxiliary1.pkl', '../TrainedModels/Model_Auxiliary2.pkl',
              '../TrainedModels/Model_Auxiliary3.pkl', '../TrainedModels/Model_Auxiliary4.pkl']

for modelName in modelPaths:
    f = open(modelName, 'rb')
    model = torch.load(f)
    f.close()

    testIn = extractWholeRecord(recordName='tr03-1371',
                                         extractAnnotations=False,
                                         dataPath='./',
                                         arousalAnnotationPath=None,
                                         apneaHypopneaAnnotationPath=None,
                                         sleepWakeAnnotationPath=None,
                                         dataInDirectory=False)
    testIn = torch.Tensor(testIn).unsqueeze(0).permute(0, 2, 1).float()

    # Test original model on GPU before cleaning
    for module in model.modules():
        # Force padding to be integer valued
        if hasattr(module, 'padding'):
            module.padding = (int(module.padding[0]),)
    print('Testing Model ' + str(modelName) + ' on GPU before cleaning')
    y0 = model(testIn.cuda())[0].detach().cpu().numpy().flatten()

    model = cleanModel(model)

    # Test original model on GPU
    print('Testing Model ' + str(modelName) + ' on GPU')
    y1 = model(testIn.cuda())[0].detach().cpu().numpy().flatten()

    with torch.no_grad():
        print('Testing Model ' + str(modelName) + ' on GPU with no grad')
        y2 = model(testIn.cuda())[0].detach().cpu().numpy().flatten()

    # Test model on cpu with no_grad
    print('Testing Model ' + str(modelName) + ' on CPU')
    model = model.cpu()
    with torch.no_grad():
        y3 = model(testIn)[0].detach().numpy().flatten()

    # Test with reconstituted model
    print('Testing Model ' + str(modelName) + ' after reconstitution')
    model2 = Sleep_model_MultiTarget()
    model2.load_state_dict(model.state_dict())
    model2.eval()
    with torch.no_grad():
        y4 = model2(testIn)[0].detach().numpy().flatten()

    print('Compare original to cleaned model: ' + str(np.max(np.abs(y0 - y1))))
    print('Compare GPU to no_grad() on GPU: ' + str(np.max(np.abs(y0-y2))))
    print('Compare GPU to CPU: ' + str(np.max(np.abs(y0-y3))))
    print('Compare Original model to Reconstituted model on CPU: ' + str(np.max(np.abs(y0-y4))))

    model2 = model2.cuda()
    model2.eval()
    y5 = model2(testIn.cuda())[0].detach().cpu().numpy().flatten()

    print('Compare Original model to Reconstituted model on GPU: ' + str(np.max(np.abs(y0-y5))))

    torch.save(model.state_dict(), modelName[0:-4] + '_stateDict.pkl')
